#!/usr/bin/env python3
"""
üöÄ OTIMIZA√á√ïES TURBO PARA GPU INTEGRADA
Acelera OpenVINO + DirectML ao m√°ximo
Vers√£o corrigida sem conflitos
"""

import os
import subprocess
import sys
import platform
import json

def print_header(text):
    print("\n" + "="*70)
    print(text)
    print("="*70)

def run_cmd(cmd, desc=""):
    if desc:
        print(f"‚è≥ {desc}...")
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=300)
        if result.stdout:
            print(f"‚úÖ {desc}")
        return True
    except subprocess.TimeoutExpired:
        print(f"‚è±Ô∏è  {desc} - Tempo esgotado")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è  {desc} - Erro: {str(e)[:100]}")
        return False

def check_gpu_type():
    """Verifica se realmente tem GPU Intel antes de instalar otimiza√ß√µes"""
    try:
        if platform.system() == 'Windows':
            result = subprocess.run(
                ['wmic', 'path', 'win32_videocontroller', 'get', 'name'],
                capture_output=True, text=True
            )
            for line in result.stdout.split('\n'):
                if 'Intel' in line and ('Iris' in line or 'UHD' in line or 'HD' in line):
                    return True
        return False
    except:
        return False

def main():
    print_header("‚ö° OTIMIZADOR TURBO - XELR√çS")
    print("\nEste script instala aceleradores extras para sua GPU integrada:")
    
    if not check_gpu_type():
        print("\n‚ö†Ô∏è  GPU Intel n√£o detectada!")
        print("Este otimizador √© espec√≠fico para GPUs Intel integradas.")
        print("Se voc√™ tem outra GPU (NVIDIA/AMD), n√£o √© necess√°rio rodar este script.")
        resposta = input("\nContinuar mesmo assim? (s/N): ").strip().lower()
        if resposta != 's':
            print("\n‚ùå Otimiza√ß√£o cancelada.")
            return
    
    venv_path = '.venv'
    if not os.path.exists(venv_path):
        print(f"\n‚ùå Ambiente virtual n√£o encontrado em {venv_path}")
        print("   Execute: python menu.py ‚Üí Op√ß√£o 2 primeiro")
        return
    
    # Define caminhos baseado no sistema operacional
    if platform.system() == 'Windows':
        pip_path = os.path.join(venv_path, 'Scripts', 'pip.exe')
        python_path = os.path.join(venv_path, 'Scripts', 'python.exe')
    else:
        pip_path = os.path.join(venv_path, 'bin', 'pip')
        python_path = os.path.join(venv_path, 'bin', 'python')
    
    if not os.path.exists(pip_path):
        print(f"\n‚ùå pip n√£o encontrado! Execute Op√ß√£o 2 do menu primeiro.")
        return
    
    print(f"\n‚úì venv encontrado: {venv_path}")
    
    # 1Ô∏è‚É£ Verifica√ß√£o de depend√™ncias b√°sicas
    print_header("1Ô∏è‚É£  VERIFICANDO DEPEND√äNCIAS B√ÅSICAS")
    
    # Garante que onnxruntime-directml est√° instalado (mais seguro que IPEX)
    if platform.system() == 'Windows':
        print("\n‚è≥ Instalando/atualizando onnxruntime-directml...")
        run_cmd([pip_path, 'install', '--upgrade', 'onnxruntime-directml'], 
                "onnxruntime-directml")
    
    # 2Ô∏è‚É£ Instalar otimiza√ß√µes leves e seguras
    print_header("2Ô∏è‚É£  INSTALANDO OTIMIZA√á√ïES SEGURAS")
    
    optimizations = [
        ('coloredlogs', 'coloredlogs'),  # Logs coloridos
        ('psutil', 'psutil'),  # Monitoramento de sistema
        ('onnx-simplifier', 'onnx-simplifier'),  # Reduz modelos ONNX
        ('--upgrade', 'torchvision'),  # Atualiza torchvision
    ]
    
    for cmd, name in optimizations:
        if cmd == '--upgrade':
            run_cmd([pip_path, 'install', '--upgrade', name], f"Atualizando {name}")
        else:
            run_cmd([pip_path, 'install', name], f"Instalando {name}")
    
    # 3Ô∏è‚É£ Teste de verifica√ß√£o
    print_header("3Ô∏è‚É£  VERIFICANDO ACELERADORES")
    
    test_code = '''
import sys
print("\\n" + "="*60)
print("üîß DIAGN√ìSTICO DE ACELERADORES")
print("="*60)

# ONNX Runtime
try:
    import onnxruntime as ort
    providers = ort.get_available_providers()
    print("ONNX Runtime Providers:")
    for p in providers:
        status = "‚úì" if p != "CPUExecutionProvider" else "‚ö†Ô∏è"
        print(f"  {status} {p}")
except Exception as e:
    print(f"‚ùå ONNX Runtime: {e}")

# PyTorch
try:
    import torch
    print(f"\\nPyTorch: {torch.__version__}")
    if hasattr(torch, 'directml'):
        print("‚úì torch-directml dispon√≠vel")
    else:
        print("‚ö†Ô∏è torch-directml n√£o dispon√≠vel")
except Exception as e:
    print(f"‚ùå PyTorch: {e}")

# OpenVINO
try:
    import openvino as ov
    print(f"\\nOpenVINO: {ov.__version__}")
    print("‚úì OpenVINO dispon√≠vel para modelos .xml")
except:
    print("\\n‚ö†Ô∏è OpenVINO n√£o instalado (opcional)")

print("\\n" + "="*60)
print("üéØ CONFIGURA√á√ÉO RECOMENDADA:")
print("  ‚Ä¢ Use modelos ONNX com DmlExecutionProvider")
print("  ‚Ä¢ Para Intel Iris: ative DirectML no menu.py")
print("  ‚Ä¢ Resolu√ß√£o: 512x512 para melhor velocidade")
print("  ‚Ä¢ Limite de LoRAs: At√© 4 simult√¢neas (estabilidade)")
print("="*60)
'''
    
    try:
        subprocess.run([python_path, '-c', test_code], check=True)
    except Exception as e:
        print(f"Erro no teste: {e}")
    
    # 4Ô∏è‚É£ Criar arquivo de configura√ß√£o otimizado
    print_header("4Ô∏è‚É£  CONFIGURA√á√ÉO OTIMIZADA")
    
    config_file = 'config_turbo.json'
    config_content = {
        "turbo_mode": {
            "enabled": True,
            "fast_generation": True,
            "reduce_vram": True,
            "optimal_resolution": "512x512",
            "max_loras": 4
        },
        "providers_priority": [
            "DmlExecutionProvider",
            "CPUExecutionProvider"
        ],
        "model_settings": {
            "openvino": {
                "compile": True,
                "device": "GPU",
                "precision": "FP16"
            },
            "onnx": {
                "enable_attention_slicing": True,
                "enable_vae_slicing": True
            }
        }
    }
    
    with open(config_file, 'w') as f:
        json.dump(config_content, f, indent=2)
    
    print(f"‚úÖ Arquivo {config_file} criado com configura√ß√µes otimizadas!")
    
    # 5Ô∏è‚É£ Dicas finais (atualizado para 4 LoRAs)
    print_header("5Ô∏è‚É£  DICAS PARA M√ÅXIMA VELOCIDADE")
    
    tips = """
üéØ CONFIGURA√á√ïES R√ÅPIDAS NO XELR√çS:

1. SEMPRE use DmlExecutionProvider (se dispon√≠vel)
   ‚Ä¢ 3-5x mais r√°pido que CPU

2. Limite de LoRAs: At√© 4 simult√¢neas
   ‚Ä¢ Mais que 4 pode causar instabilidade
   ‚Ä¢ Combine for√ßas: 0.5-1.0 geralmente √© suficiente

3. Reduza resolu√ß√£o inicialmente:
   ‚Ä¢ 512x512 ‚Üí Mais r√°pido, boa qualidade
   ‚Ä¢ 768x768 ‚Üí Balanceado
   ‚Ä¢ 1024x1024 ‚Üí S√≥ se necess√°rio

4. Ajuste passos (steps):
   ‚Ä¢ R√°pido: 15-20 steps
   ‚Ä¢ Qualidade: 25-30 steps
   ‚Ä¢ M√°ximo: 40-50 steps (raro)

5. CFG Scale:
   ‚Ä¢ 6.0-7.0: Mais r√°pido, criativo
   ‚Ä¢ 7.0-8.0: Balanceado
   ‚Ä¢ 8.0-9.0: Segue prompt rigorosamente

‚ö° DICAS COM LORAS:

‚Ä¢ Comece com 1 LoRA, depois adicione mais
‚Ä¢ For√ßa total (1.0) nem sempre √© melhor
‚Ä¢ Teste combina√ß√µes: personagem + estilo + cor
‚Ä¢ M√°ximo recomendado: 2-3 LoRAs para estabilidade

üìä MONITORAMENTO:
‚Ä¢ Task Manager ‚Üí Performance ‚Üí GPU
‚Ä¢ Deve mostrar 70-90% uso durante gera√ß√£o
‚Ä¢ Se mostra 100% CPU, algo est√° errado
"""
    
    print(tips)
    
    print_header("‚úÖ OTIMIZA√á√ÉO CONCLU√çDA!")
    print("\nAgora execute:")
    print("1. python menu.py")
    print("2. Op√ß√£o 1 (Iniciar Xelr√≠s)")
    print("3. Selecione DmlExecutionProvider quando perguntado")
    print("\n‚ö° Suas gera√ß√µes ser√£o muito mais r√°pidas!")
    print("üé® Limite de LoRAs: At√© 4 simult√¢neas para melhor estabilidade\n")

if __name__ == "__main__":
    main()